% if you want the answers to appear uncomment the below
\documentclass[answers]{exam}
% otherwise uncomment the below
%\documentclass{exam}
\usepackage[bottom]{footmisc}
\usepackage{graphicx}
\usepackage[letterpaper, margin=.9in]{geometry}
\usepackage{natbib}
\bibpunct{(}{)}{;}{a}{,}{,}
\usepackage{url}
\def\UrlFont{\rm}


\usepackage{Sweave}

\usepackage[utf8x]{inputenc}
\usepackage{array}
\usepackage{verbatim}
\usepackage{amsfonts}
\usepackage{amsmath}
%\usepackage{lineno}
\setlength{\parskip}{2ex} 
%\setlength{\parindent}{0ex}

% Cannot place floats in figure environment.
\usepackage{caption}
\usepackage{newfloat}
%\DeclareCaptionListFormat{myliststyle}{#1.#2}
\DeclareCaptionType{mytype}[Fig.][List of mytype]
\newenvironment{myfigure}{\captionsetup{type=mytype}}{}



\newenvironment{packed_enum}{
\begin{enumerate}
 \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}
\newenvironment{packed_item}{
\begin{itemize}
 \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

 \newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
    \def\independenT#1#2{\mathrel{\setbox0\hbox{$#1#2$}%
    \copy0\kern-\wd0\mkern4mu\box0}} 


\bibliographystyle{plainnat}

\pagestyle{myheadings}
\markright{Advanced Topics in Causal Inference \hfill  R Lab \#3 \hfill}


\title{R Lab 3 - Understanding Time Dependent Confounding and Identifiability in Longitudinal Context}
\author{Advanced Topics in Causal Inference}
\date{}

\begin{document}
\maketitle

\input{RLab3_answers-concordance}

\noindent \textbf{Assigned:} September 21, 2021\\
\textbf{Lab due:} September 28, 2020 on bCourses. Please answer all questions and include relevant \texttt{R} code. You are encouraged to discuss the assignment in groups, but should not copy code or interpretations verbatim. Upload your own completed lab to bCourses.



\noindent \textbf{Last lab:} \\
Translate causal questions into target causal parameters and intervene on the data generating processes described by Structural Causal Models (SCMs) to evaluate the true value of these parameters. 


\noindent \textbf{This lab:}\\
1. Review the concept of identifiability. \\
2. Determine which assumptions let us achieve identifiability. \\
3. Write the target causal parameter (a function of the counterfactual or ``post intervention" distribution) as a function of the observed data distribution. \\
4. Obtain the value of the statistical estimand. \\
5. Understand the challenges posed by time-dependent confounding.

\noindent \textbf{Next lab:}\\
Implementation of longitudinal IPTW to estimate the intervention specific mean and the parameters of a marginal structural model.



\begin{center}
\noindent\rule{18cm}{0.4pt}
\end{center}

\section{Introduction and Motivation}

In previous labs we examined the counterfactual distributions (and parameters of these distributions) of the variables that help us answer our causal questions of interest. 

\noindent In reality, we won't know the counterfactual distribution (e.g., we can't observe exogenous errors or counterfactual outcomes, and we don't know the true data generating process). Instead, we see instances of data generated from the \textit{observed} data distribution.

\noindent So, we now would like to be able to write our target causal parameter as a parameter of our observed data distribution $P_0$. If we can do this, we have achieved \textit{identifiability}. Having defined a target statistical parameter (or estimand), we can estimate it using our observed data (typically in this class, $n$ i.i.d. copies of the observed random variable $O$; coming up in the next lab).

\begin{myfigure}
\begin{center}
\includegraphics[width=.3\textwidth]{identification.jpg}
\caption{Identification.}
\end{center}
\end{myfigure}

\section{This lab}

Last time, you wrote down target causal parameters inspired by the causal questions posed to you by your GSR. However, in order to actually evaluate effects of sleep on health and performance outcomes, you'll need to come up with a function of the distribution of the observed data that your professor hands to you. 

%In Causal I, you learned the approach to identifying a target causal parameter as a parameter of the observed data distribution in a point-treatment setting. Specifically, Causal I focused on the randomization assumption/ backdoor criterion. Under this assumption (or adequate data support), causal parameters defined by interventions at a single time point were identified using the point treatment g-computation formula.

\noindent Since you are dealing with studies in which you are interested in the effects of multiple interventions (e.g., cumulative sleep over time), to achieve identifiability, you will need to consider assumptions beyond ones you've learned in the point-treatment setting. 


\noindent The first data structure works off of new variable definitions and data generating processes. The last three data structures build off of previous labs: refer back to \texttt{R} Lab 1 for variable definitions and specific data generating processes for data structures 1, 2, and 4.

\subsection{To turn in:}


\noindent\fbox{
    \parbox{\textwidth}{

\textbf{\textit{Note: there are separate questions for Data Structure 0}} \\

\textbf{\underline{For Data Structures 1, 2, and 4, answer the following questions:}} \\

\begin{enumerate}
\item \textbf{Is the true $P_{U,X}$ an element of (i.e., described by or compatible with) the SCM presented?} Refer back to \texttt{R} Lab 1 for the true $P_{U,X}$.
\item We will present a target causal parameter. \textbf{Is the target causal parameter (a parameter of $P_{U,X}$) identified (as a parameter of $P_0$) under the standard, point treatment randomization assumption/back door criteria? Why or why not?}
\item \textbf{If the target parameter is not identified in the previous question, what are the alternative assumptions under which the parameter would be identified?}
\item \textbf{What is the corresponding statistical estimand, $\Psi(P_0)$, under these assumptions?}
\end{enumerate}


    }
}

\pagebreak
\noindent\large\textbf{Data Structure 0: $O = (L(1), A(1), L(2), A(2), Y)$}
\normalsize

Recall that in Causal I, we learned a common identification result for the expectation of the counterfactual outcome under a \textit{point treatment intervention} (such as the average treatment effect). Specifically, for $O = (W, A, Y)$, under the randomization assumption ($Y_a \perp A|W$), or if $W$ satisfied the back door criteria with respect to the effect of $A$ on $Y$, (together with the positivity assumption) we had the following result:
\[
E[Y_a] = \sum_{w}E_0[Y|A=a,W=w] \times P_0(W=w)
\]

\vspace{-4mm}
\noindent The right hand side of the equation is called the \textit{point-treatment g-computation formula}. 
\vspace{3mm}

In the following example we use a simple discrete data structure to illustrate how this identification result can break down in the \textit{longitudinal} setting -- that is, when we are interested in evaluating the effects of interventions on more than one variable. 


\noindent\underline{SCM}:

$U=(U_{\bar{L}(2)}, U_{\bar{A}(2)}, U_Y) \sim P_U$ , where we assume all $U$s are independent. 

$X=(L(1), A(1), L(2), A(2), Y)$ 

and $f_X$ is:

\begin{align*}
L(1)&=f_{L(1)}(U_{L(1)}) \\
A(1)&=f_{A(1)}(U_{A(1)}, L(1)) \\
L(2)&=f_{L(2)}(U_{L(2)}, L(1), A(1)) \\
A(2)&=f_{A(2)}(U_{A(2)}, \bar{L}(2)) \\
Y&=f_{Y}(U_{Y}, \bar{L}(2), \bar{A}(2))\\
\end{align*}

\noindent \underline{$P_{U,X}$} (implying one particular joint distribution of $(U, X)$, in other words, is an element of the above SCM):
\begin{align*}
\text{Exogenous} & \text{ variables:} \\
U_{L(t)} \text{ for } t = 1,2 & \sim Uniform(min=0, max=1) \\
U_{A(t)} \text{ for } t = 1,2 &\sim Uniform(min=0, max=1) \\
U_{Y} &\sim Uniform(min=0, max=1) \\
\text{Structural } & \text{equations $F$ and endogenous variables:}  \\
L(1)& = \mathbb{I}[U_{L(1)}<0.5] \\
A(1)& = \mathbb{I}[U_{A(1)}<expit(0.3-L(1))]\\
L(2)& = \mathbb{I}[U_{L(2)}<expit(-2+1.8^*A(1)+2^*L(1))] \\
A(2)& = \mathbb{I}[U_{A(2)}<expit(L(2)+L(1))] \\
Y& = \mathbb{I}[U_{Y}<expit(-3+1.3^*A(1)+1.7^*A(2)+1.3^*L(1)+1.7^*L(2))]\\
\end{align*}
And the true value of the target causal parameter of interest is (optional: verify this yourself!):
\[
\Psi^F(P_{U,X}) = E_{U,X}[Y_{\bar{a}=1}] = 0.7921
\]

\noindent Specifically, below we'll show that the point treatment g-computation formula (which conditions on either $L(1)$ and $L(2)$, or on $L(1)$ only) \underline{does not equal} the true value of our longitudinal target causal parameter:

\vspace{2mm}

\begin{enumerate}
\item \textbf{Generate a large amount of data according to $P_{U,X}$, above.} In \texttt{R}, generate a large number of observations of $O$ (say, $n = 1,000,000$) according to the above data-generating process. 

\item \textbf{If we use the \textit{point-treatment} g-computation formula and condition on $L(1)$ and $L(2)$, what is the statistical estimand $\Psi^{(1)}(P_0)$ equal to? Is it equal to the true value of the target causal parameter?}
\[
\Psi^{(1)}(P_0) = \sum_{\bar{l}(2)}E_0[Y|\bar{A}(2) = 1, \bar{L}(2) = \bar{l}(2)] \times P_0\big{(}\bar{L}(2) = \bar{l}(2)\big{)} = \text{ ???}
\]
\begin{enumerate}
\item Notice that the sum of $\Psi^{(1)}(P_0)$ is over every permutation of $\bar{L}(2) = \bar{l}(2)$: 
\begin{align*}
\text{Permutation 1} = (L(1) = 1, &L(2) = 1) \\
\text{Permutation 2} = (L(1) = 1, &L(2) = 0) \\
\text{Permutation 3} = (L(1) = 0, &L(2) = 1) \\
\text{Permutation 4} = (L(1) = 0, &L(2) = 0) \\
\end{align*}

For each of these permutations we need to compute: 
$$E_0[Y|\bar{A}(2) = 1, \bar{L}(2) = \bar{l}(2)] \times P_0\big{(}\bar{L}(2) = \bar{l}(2)\big{)}$$ 
We'll demonstrate how to do this for Permutation 1. Then, you can repeat this derivation for all other permutations, using Permutation 1 as a model. Let's break this quantity under Permutation 1 in two parts:
$$\underbrace{E_0[Y|\bar{A}(2) = 1, \bar{L}(2) = 1]}_\text{\textbf{Part 1}} \times \underbrace{P_0\big{(}\bar{L}(2) = 1\big{)}}_\text{\textbf{Part 2}}$$


\vspace{4mm}
\begin{itemize}
\item[\textbf{Part}] \textbf{1} Compute $E_0[Y|\bar{A}(2)=1, \bar{L}(2) = 1]$ by subsetting the values of \texttt{Y} for which \texttt{A1, A2, L1,} \textit{and} \texttt{L2} are 1, and take the mean:
\vspace{2mm}
\begin{Schunk}
\begin{Sinput}
> mean(Y[A1 == 1 & A2 ==1 & L1 == 1 & L2 == 1])
\end{Sinput}
\end{Schunk}
\vspace{4mm}
\item[\textbf{Part}] \textbf{2} Compute $P_0\big{(}\bar{L}(2) = 1\big{)}$ by obtaining the proportion of times where \texttt{L1} \textit{and} \texttt{L2} are 1: 
\vspace{2mm}
\begin{Schunk}
\begin{Sinput}
> mean(L1 == 1 & L2 == 1)
\end{Sinput}
\end{Schunk}
\vspace{4mm}
\end{itemize}
\item Multiply \textbf{Part 1} and \textbf{Part 2} together.
\item Repeat this process for all other permutations of $\bar{L}(2)$. Then, sum  all of these quantities together to obtain the statistical estimand, $\Psi^{(1)}(P_0)$.
\item Is $\Psi^{(1)}(P_0)$ equal to $\Psi^F(P_{U,X})$? Have we achieved identifiability?
\end{enumerate}


\item \textbf{If we use the \textit{point-treatment} g-computation formula and condition on $L(1)$, what is the statistical estimand $\Psi^{(2)}(P_0)$ equal to? Is it equal to the true value of the target causal parameter?} Solve $\Psi^{(2)}(P_0)$ computationally; in other words, use \texttt{R} to evaluate $\Psi^{(2)}(P_0)$, as in the previous problem. \textit{Hint: be sure to sum over the correct permutations!} 
\[
\Psi^{(2)}(P_0) = \sum_{l(1)}E_0[Y|\bar{A}(2) = 1, L(1) = l(1)] \times P_0\big{(}L(1) = l(1)\big{)} = \text{ ???}
\]



\item \textbf{If we use the \textit{longitudinal} g-computation formula, what is the statistical estimand $\Psi^{(3)}(P_0)$ equal to? Is it equal to the true value of the target causal parameter?} 
\[
\Psi^{(3)}(P_0) = \sum_{\bar{l}(2)}E_0[Y | \bar{A}(2)=1, \bar{L}(2) = \bar{l}(2)] \times P_0\big{(}L(2) = l(2) | A(1)=1, L(1) = l(1)\big{)} \times P_0(L(1) = l(1)\big{)} =  \text{ ???}
\]

\begin{enumerate}

\item \textbf{Solve $\Psi^{(3)}(P_0)$ computationally.} In other words, use \texttt{R} to evaluate $\Psi^{(3)}(P_0)$ (as in the previous two problems).

\item \textbf{Bonus!} Solve $\Psi^{(3)}(P_0)$ analytically. Again, notice that the sum of $\Psi^{(3)}(P_0)$ is over every permutation of $\bar{L}(2) = \bar{l}(2)$. We'll demonstrate how to get started for $(L(1) = 1, L(2) = 1)$. Then, you can repeat this derivation for all other permutations of $\bar{L}(2)$. First, let's break $\Psi^{(3)}(P_0)$ into three parts:
\[
\Psi^{(3)}(P_0) = \sum_{\bar{l}(2)}\underbrace{E_0[Y | \bar{A}(2)=1, \bar{L}(2) = \bar{l}(2)]}_\text{\textbf{Part 1}} \times \underbrace{P_0\big{(}L(2) = l(2) | A(1)=1, L(1) = l(1)\big{)}}_\text{\textbf{Part 2}} \times \underbrace{P_0(L(1) = l(1)\big{)}}_\text{\textbf{Part 3}}
\]


\begin{enumerate}
\item[\textbf{Part}]\textbf{1} Compute $E_0[Y|\bar{A}(2)=1, \bar{L}(2) = 1]$:

\begin{align*}
E[Y|\bar{A}(2)=1, \bar{L}(2) = 1]  = E[\mathbb{I}[U_{Y}<expit(& -3+1.3^*A(1)+1.7^*A(2) \\
& + 1.3^*L(1)+1.7^*L(2)] \big{|} \bar{A}(2)=1, \bar{L}(2) = 1] \\
\intertext{The expectation of an indicator is just the probability that indicator equals 1:}
 =  P\Big{(}U_{Y}<expit\big{(}& -3+1.3^*A(1)+1.7^*A(2)\\
 & +1.3^*L(1)+1.7^*L(2)\big{)} \big{|} \bar{A}(2)=1, \bar{L}(2) = 1 \Big{)} \\
\intertext{Substitute in the constant values we're conditioning on:}
 = P\Big{(}U_{Y}<expit\big{(}& -3+1.3^*1+1.7^*1 +1.3^*1+1.7^*1\big{)}\Big{)} \\
\intertext{The probability a uniform random variable between $0$ and $1$ is less than a constant value is just that constant value:}
 = expit\big{(}&-3+1.3^*1+1.7^*1+ 1.3^*1+1.7^*1\big{)} \\
\end{align*}
\item[\textbf{Part}]\textbf{2} Compute $P_0(L(2) = 1 | A(1) = 1, L(1) = 1)$:
\begin{align*}
P(L(2) = 1 | A(1) = 1, L(1) = 1) & = P\Big{(}\mathbb{I}[U_{L(2)}<expit(-2+1.8^*A(1)+2^*L(1)) \big{|} A(1) = 1, L(1)] = 1\Big{)} \\
& =  P\Big{(}\mathbb{I}[U_{L(2)}<expit(-2+1.8^*1+2^*1)] = 1\Big{)} \\
& =  expit(-2+1.8^*1+2^*1)
\end{align*}
\item[\textbf{Part}]\textbf{3}  Compute $P_0(L(1) = 1)$:
\begin{align*}
P(L(0) = 1) & = P\big{(}\mathbb{I}[U_{L(1)} < 0.5] = 1\big{)} = 0.5 \\
\end{align*}

\end{enumerate}

Multiply \textbf{Part 1}, \textbf{Part 2}, and \textbf{Part 3} together.

\vspace{3mm}

\noindent Repeat this process for all other permutations of $\bar{L}(2)$. Then, sum over all of these quantities together to obtain the statistical estimand, $\Psi^{(3)}(P_0)$.

\end{enumerate}


\end{enumerate}

\begin{solution}

\begin{enumerate}

\item Solve $\Psi^{(1)}(P_0)$ (recall that in this scenario, we are conditioning on $L(1)$ and $L(2)$):


\begin{Schunk}
\begin{Sinput}
> # set the seed
> set.seed(252)
\end{Sinput}
\end{Schunk}
\begin{Schunk}
\begin{Sinput}
> # number of times to generate
> n = 1000000
\end{Sinput}
\end{Schunk}
\begin{Schunk}
\begin{Sinput}
> # print data generating process for data structure 0
> print(generate_data0)
\end{Sinput}
\begin{Soutput}
function(n){
  
  L1 = rbinom(n,1,0.5)
  A1 = rbinom(n,1,plogis(0.3-L1))
  L2 = rbinom(n,1,plogis(-2+1.8*A1+2*L1))
  A2 = rbinom(n,1,plogis(L2+L1))
  Y = rbinom(n,1,plogis(-3+1.3*A1+1.7*A2+1.3*L1+1.7*L2))
  
  O = data.frame(L1, A1, L2, A2, Y)
  return(O)
  
}
<bytecode: 0x7ff106eda440>
\end{Soutput}
\end{Schunk}
\begin{Schunk}
\begin{Sinput}
> # save to dataframe ObsData0
> ObsData0 = generate_data0(n)
\end{Sinput}
\end{Schunk}
\begin{Schunk}
\begin{Sinput}
> # extract variables
> L1 = ObsData0$L1
> A1 = ObsData0$A1
> L2 = ObsData0$L2
> A2 = ObsData0$A2
> Y = ObsData0$Y
\end{Sinput}
\end{Schunk}
\begin{Schunk}
\begin{Sinput}
> # calculate Psi1(P0)
> Psi1.P0 = mean(Y[A1 == 1 & A2 ==1 & L1 == 1 & L2 == 1])*mean(L1 == 1 & L2 == 1) + 
+   mean(Y[A1 == 1 & A2 ==1 & L1 == 1 & L2 == 0])*mean(L1 == 1 & L2 == 0) + 
+   mean(Y[A1 == 1 & A2 ==1 & L1 == 0 & L2 == 1])*mean(L1 == 0 & L2 == 1) + 
+   mean(Y[A1 == 1 & A2 ==1 & L1 == 0 & L2 == 0])*mean(L1 == 0 & L2 == 0) 
> Psi1.P0
\end{Sinput}
\begin{Soutput}
[1] 0.7477152
\end{Soutput}
\end{Schunk}
$\Psi^{(1)}(P_0) = $ 0.7477. Here, $\Psi^{(1)}(P_0) \neq \Psi^F(P_{U,X})$; we have underestimated the target causal parameter. 

\item Solve $\Psi^{(2)}(P_0)$ (recall that in this scenario, we are conditioning on $L(1)$ only):
\begin{Schunk}
\begin{Sinput}
> # calculate Psi2(P0)
> Psi2.P0 = mean(Y[A1 == 1 & A2 ==1 & L1 == 1])*mean(L1 == 1) + 
+   mean(Y[A1 == 1 & A2 ==1 & L1 == 0])*mean(L1 == 0) 
> Psi2.P0
\end{Sinput}
\begin{Soutput}
[1] 0.8103919
\end{Soutput}
\end{Schunk}
$\Psi^{(2)}(P_0) = $ 0.8104. Again, $\Psi^{(2)}(P_0) \neq \Psi^F(P_{U,X})$, and we have overestimated the target causal parameter. 

\item Solve for $\Psi^{(3)}(P_0)$ (using longitudinal g-computation formula):
\begin{enumerate}
\item Computationally:
\begin{Schunk}
\begin{Sinput}
> EY.11 = mean(Y[A1 == 1 & A2 ==1 & L1 == 1 & L2 == 1])*
+   mean(L2[A1 == 1 & L1 == 1])*
+   mean(L1)
> EY.10 = mean(Y[A1 == 1 & A2 ==1 & L1 == 1 & L2 == 0])*
+   (1 - mean(L2[A1 == 1 & L1 == 1]))*
+   mean(L1)
> EY.01 = mean(Y[A1 == 1 & A2 ==1 & L1 == 0 & L2 == 1])*
+   mean(L2[A1 == 1 & L1 == 0])*
+   (1 - mean(L1)) 
> EY.00 = mean(Y[A1 == 1 & A2 ==1 & L1 == 0 & L2 == 0])*
+   (1 - mean(L2[A1 == 1 & L1 == 0]))*
+   (1 - mean(L1))
> Psi3.P0 = EY.11 + EY.10 + EY.01 + EY.00
> Psi3.P0
\end{Sinput}
\begin{Soutput}
[1] 0.792322
\end{Soutput}
\end{Schunk}
\item Analytically: \\
Continuing for all other permutations of $(L(1), L(2))$...
\begin{itemize}
\item[-] Case where $(L(1) = 1, L(2) = 1))$: 
\begin{align*}
& E[Y|\bar{A} = 1, L(1) = 1, L(2) = 1]P(L(2) = 1 | A(1) = 1, L(1) = 1)P(L(1) = 1) \\
& = expit(3)^*expit(1.8)^*0.5 \\
& = 0.4087
\end{align*}
\item[-] Case where $(L(1) = 1, L(2) = 0))$:
\begin{align*}
& E[Y|\bar{A} = 1, L(1) = 1, L(2) = 0]P(L(2) = 0 | A(1) = 1, L(1) = 1)P(L(1) = 1) \\
& = expit(1.3)^*(1-expit(1.8))^*0.5 \\
& = 0.0557
\end{align*}
\item[-] Case where $(L(1) = 0, L(2) = 1))$:
\begin{align*}
& E[Y|\bar{A} = 1, L(1) = 0, L(2) = 1]P(L(2) = 1 | A(1) = 1, L(1) = 0)P(L(1) = 0) \\
& = expit(1.7)^*expit(-0.2)^*0.5\\
& = 0.1903
\end{align*}
\item[-] Case where $(L(1) = 0, L(2) = 0))$:
\begin{align*}
& E[Y|\bar{A} = 1, L(1) = 0, L(2) = 0]P(L(2) = 0 | A(1) = 1, L(1) = 0)P(L(1) = 0) \\
& = expit(0)^*(1-expit(-0.2))^*0.5\\
& = 0.1375
\end{align*}
\end{itemize}

Adding these components together we get $\Psi^{(3)}(P_0) = $ 0.7922 
\end{enumerate}

We see that $\Psi^{(3)}(P_0)$, which uses the longitudinal g-computation formula, does equal the value of our target causal estimand (assume any difference is due to rounding error). Under the sequential randomization and positivity assumption, this statistical estimand identifies our causal parameter of interest.

Note that the above does not constitute a proof of this identification result - identification implies that this equality will hold for every $P_{U,X}$ compatible with the SCM, and every $P_0$ implied by this $P_{U,X}$. We have just shown that it holds for one such $P_{U,X}$.

\end{enumerate}
\end{solution}

\pagebreak
\noindent\large\textbf{Data Structure 1: $O = (W, A, L, \Delta, \Delta Y)$}
\normalsize

\begin{enumerate}
\item \underline{SCM}:
\begin{packed_item}
\item[] $U = (U_{W}, U_{A}, U_{L}, U_{\Delta}, U_{Y}) \sim P_U$. Assume $U$s are jointly independent.
\item[] Structural equations, $F$:
\begin{align*}
W& = f_{W}(U_{W}) \\
A & = f_A(W, U_{A}) \\
L & = f_L(W, A, U_{L}) \\
\Delta & = f_{\Delta}(W, A, L, U_{\Delta}) \\
Y & = f_{Y}(W, A, L, U_{Y})
\end{align*}
\end{packed_item}
\textbf{Is the true $P_{U,X}$ described by the SCM presented?} Refer back to the first data structure of \texttt{R} Lab 1 for the true $P_{U,X}$.
\item \underline{Target causal parameter}:
\[
\Psi^F(P_{U,X}) = E_{U,X}[Y_{a=0, \Delta=1}]
\]
\textbf{Is this target causal parameter (a parameter of $P_{U,X}$) identified (as a parameter of $P_0$) under the standard, point treatment randomization assumption/back door criteria? Why or why not?}
\item \textbf{If the target parameter is not identified in the previous question, what are the alternative assumptions under which the parameter would be identified?} 
\item \textbf{What is the corresponding statistical estimand, $\Psi(P_0)$, under these assumptions?}
\end{enumerate}










\begin{solution}
\begin{enumerate}
\item $P_{U,X}$ is described by the SCM. An SCM is a model on the set of possible data generating processes. The data generating process (i.e., $P_{U,X}$) we wrote down in \texttt{R} Lab 1 is an element (a specific instance) of our SCM written above.

\item The point treatment backdoor criteria here would require one set of covariates that blocks backdoor paths from the joint intervention $(A, \Delta)$ to $Y$ \textit{and} does not include descendants of $(A, \Delta)$. Our two options are:

\begin{itemize}
\item[-] Conditioning on $W$. Here, the backdoor criteria fails because there is a backdoor path from $\Delta \leftarrow L \rightarrow Y$ and $(Y_{a=0, \Delta =1} \not\!\perp\!\!\!\perp (A, \Delta) | W$). Thus, our target causal parameter $E_{U,X}[Y_{a=0, \Delta = 1}] \neq E_0[E_0[Y|A = 0, \Delta = 1, W]]$. Informally, if we fail to adjust for $L$, we have informative missingness.
\item[-] Conditioning on $W$ and $L$. Here, the backdoor criteria fails because $L$ is affected by $A$. Thus, our target causal parameter $E_{U,X}[Y_{a=0, \Delta = 1}] \neq E_0[E_0[Y|A=0,\Delta=1,W,L]]$. This is because the right hand side of the inequality is integrating with respect to the observed distribution of $L$, rather than the counterfactual distribution under an intervention to set $A=0$. Informally, if we just condition on $L$ in a single regression, we lose part of the effect of $A$ on $Y$.
\end{itemize}

Thus, the target causal parameter is \textit{not} identified using the point treatment g-computation formula.

\item Alternative assumptions sufficient to identify the target causal parameter:

\begin{itemize}
\item[-] Sequential randomization assumption (and corresponding sequential backdoor criteria): 
\begin{align*}
Y_{a=0,\Delta = 1} & \perp A|W \\
Y_{a=0, \Delta = 1} & \perp \Delta | W, L, A=0
\end{align*}
\item[-] Positivity assumption
\begin{align*}
P_0(A = 0|W = w) & > 0 - a.e.\\
P_0(\Delta = 1 | W = w, L = l, A = 0) & > 0 - a.e. \\
\end{align*}
\end{itemize}

\item Under the above assumptions, the target causal parameter is equal to a function of the observed data distribution. Specifically:
\begin{align*}
E_{U,X}[Y_{a=0, \Delta = 1}] & = \sum_{w, l}E_0[Y|A=0,\Delta=1,W=w,L=l]P_0(L=l|A=0,W=w)P_0(W=w) \\
\end{align*}


\end{enumerate}

\end{solution}

\noindent\large\textbf{Data Structure 2: $O = (L(1), A(1), L(2), A(2), L(3), A(3), L(4), A(4), Y)$}
\normalsize

\begin{enumerate}
\item \underline{SCM}:
\begin{packed_item}
\item[] $U = (U_{L(t)}, U_{A(t)}, U_{Y}), t=1,...,4 \sim P_U$. Assume $U$s are jointly independent.
\item[] Structural equations, $F$:
\begin{align*}
L(1) & = f_{L(1)}(U_{L(1)}) \\
A(1) & = f_{A(1)}(L(1), U_{A(1)}) \\
L(t) & = f_{L(t)}(\bar{L}(t-1), \bar{A}(t-1), U_{L(t)}) \text{ for } t = 2,...,4\\
A(t) & = f_{A(t)}(\bar{A}(t-1), \bar{L}(t), U_{A(t)}) \text{ for } t = 2, ..., 4 \\
Y & = f_Y(\bar{L}(4), \bar{A}(4), U_Y) 
\end{align*}
\end{packed_item}
\textbf{Is the true $P_{U,X}$ compatible with the SCM presented?} Refer back to the second data structure of \texttt{R} Lab 1 for the true $P_{U,X}$.
\item \underline{Target causal parameter}:
\[
\Psi^F(P_{U,X}) = E_{U,X}[Y_{\bar{a}(4)}]
\]
\textbf{Is the target causal parameter (a parameter of $P_{U,X}$) identified (as a parameter of $P_0$) under the standard, point treatment randomization assumption/back door criteria? Why or why not?}
\item \textbf{If the target parameter is not identified in the previous question, what are the alternative assumptions under which the parameter would be identified?}
\item \textbf{What is the corresponding statistical estimand, $\Psi(P_0)$, under these assumptions?}
\item \textbf{Bonus!} Suppose instead your target causal parameter is $E_{U,X}[Y_{a(1)}]$. What is the interpretation of this target parameter? What assumptions are needed for identifiability here? What is the statistical estimand?
\end{enumerate}



\begin{solution}

\begin{enumerate}

\item The $P_{U,X}$ is compatible with the SCM. See the previous data structure for reasoning.

\item The reasoning here is the same as the previous two data structures. We can't condition on one set of covariates to achieve identifiability -- we would either be left with unblocked backdoor paths or a loss of part of the effect of interest. For example, we need to condition on $L(3)$ to block the backdoor path from $A(3)$ to $Y$, but $L(3)$ is a descendant of $A(2)$ so we lose part of the effect of $A(2)$ on $Y$ via $L(3)$.

\item Additional assumptions needed for identifiability:

\begin{itemize}
\item[-] Sequential randomization assumption:

\[
Y_{\bar{a}(t)} \independent A(t) | \bar{L}(t), \bar{A}(t-1) = \bar{a}(t-1) \text{ for } t = 1, ..., 4 
\]

\item[-] Positivity:

\[
P_0(A(t) = a(t) | \bar{A}(t-1) = \bar{a}(t-1), \bar{L}(t)) > 0, \text{ for } t = 1,..., 4
\]

\end{itemize}

\item Under the above assumptions, the target causal parameter is equal to:

$$E_{U,X}[Y_{\bar{a}(4)}] = \sum_{\bar{l}(4)} E_0[Y|\bar{A}(4) = \bar{a}(4), \bar{L}(4) = \bar{l}(4)] \times \prod_{t = 1}^4 P_0(L(t) = l(t)|\bar{A}(t-1) = \bar{a}(t-1), \bar{L}(t-1) = \bar{l}(t-1))$$

\end{enumerate}

\noindent\textbf{Bonus:} If our target parameter is instead $E_{U,X}[Y_{a(1)}]$, this is the expectation of the counterfactual outcome (test score) under an intervention on the first night's sleep only (i.e., one night of either 8 or more hours of sleep or less than 8 hours of sleep at time 1) on the outcome. Subsequent values of $A$ are left random (i.e., they are generated without any further intervention on the data generating system). Contrasting, say $E[Y_{a(1) = 1} - Y_{a(1) = 0}]$ is thus capturing a single point treatment effect of early sleep (in part mediated by future nights' sleep), but is no longer summarizing the cumulative effect of sleep over multiple nights.

Here, we can actually use the standard point treatment randomization assumption -- we only need to condition on one set of covariates (in fact, only one covariate) to achieve the backdoor criteria. That is:

\[
Y_{a(1)} \independent A(1) | L(1)
\]

Positivity assumption:

\[
P_0(A(1) = a(1) | L(1)) > 0 - a.e.
\]

Under these assumptions, the target causal parameter is identified using the standard g-computation formula you learned in Causal I:

\[
E_{U,X}[Y_{a(1)}]=E_{L(1)}[E_0[Y|A(1)=a(1),L(1)]]
\]

\end{solution}

\noindent\large\textbf{Data Structure 4: $O = (L(1), C(1), A(1), Y(2), L(2), C(2), A(2), Y(3))$}
\normalsize


\begin{enumerate}
\item \underline{SCM}:
\begin{packed_item}
\item[] $U = (U_{L(t)}, U_{C(t)}, U_{A(t)}, U_{Y(t+1)}), t=1,2 \sim P_U$. Assume $U$s are jointly independent.
\item[] Structural equations, $F$:
\begin{align*}
L(1) & = f_{L(1)}(U_{L(1)}) \\
C(1) & = f_{C(1)}(L(1), U_{C(1)}) \\
A(1) & = f_{A(1)}(L(1), C(1), U_{A(1)}) \\
Y(2) & = f_{Y(2)}(L(1), C(1), A(1), U_{Y(2)}) \\
L(2) & = f_{L(2)}(L(1), C(1), A(1), Y(2), U_{L(2)}) \\
C(2) & = f_{C(2)}(\bar{L}(2), C(1), A(1), Y(2), U_{C(2)}) \\
A(2) & = f_{A(2)}(\bar{L}(2), \bar{C}(2), A(1), Y(2), U_{A(2)}) \\
Y(3) & = f_{Y(3)}(\bar{L}(2), \bar{C}(2), \bar{A}(2), Y(2), U_{Y(3)}) 
\end{align*}
\end{packed_item}
\textbf{Is the true $P_{U,X}$ an element of the SCM presented?} Refer back to the fourth data structure in \texttt{R} Lab 1 for the true $P_{U,X}$.
\item \underline{Target causal parameter}:
\[
\Psi^F(P_{U,X}) = E_{U,X}[Y(3)_{\bar{a}(2)=1, \bar{c}(2)=0}]
\]
\textbf{Is the target causal parameter (a parameter of $P_{U,X}$) identified (as a parameter of $P_0$) under the standard, point treatment randomization assumption/back door criteria? Why or why not?}
\item \textbf{If the target parameter is not identified in the previous question, what are the alternative assumptions under which the parameter would be identified?}
\item \textbf{What is the corresponding statistical estimand, $\Psi(P_0)$, under these assumptions?}
\end{enumerate}








\begin{solution}
\begin{enumerate}
\item $P_{U,X}$ is contained in the SCM. See data structure 1 for reasoning.
\item The point treatment backdoor criteria is not sufficient here -- there is no one set of covariates that sufficiently blocks backdoor paths from our intervention $\bar{A}(2)$ to $Y$ and are non-descendants of $A(1)$ and $A(2)$.
\item Additional assumptions:
\begin{itemize}
\item[-] Sequential randomization assumption:
\begin{align*}
Y(3)_{\bar{a}=1, \bar{c}=0} &\independent C(t) | \bar{L}(t), \bar{A}(t-1) = 1, \bar{C}(t-1) = 0, Y(t) = 0, \text{ for } t = 1, 2 \\
Y(3)_{\bar{a}=1, \bar{c}=0} &\independent A(t) | \bar{C}(t) = 0, \bar{L}(t), \bar{A}(t-1) = 1, Y(t) = 0, t = 1, 2 
\end{align*}
\item[-] Positivity assumption:
\begin{align*}
P_0(A(t)=1 &| \bar{C}(t)=0, \bar{L}(t), Y(t) = 0, \bar{A}(t-1)=1)>0 \text{ for } t=1,2 \\
P_0(C(t) = 0 &| \bar{L}(t), \bar{A}(t-1)=1, \bar{C}(t-1) =0, Y(t)=0) > 0 \text{ for } t = 1, 2 
\end{align*}
\end{itemize}

Equivalently (since $A(t)$ and $C(t)$ are adjacent nodes), we can write the assumptions as follows:
\begin{itemize}
\item[-] Sequential randomization assumption:
\[
Y(3)_{\bar{a}=1,\bar{c}=0} \independent (A(t),C(t)) | \bar{L}(t),Y(t)=0,C(t-1)=0,\bar{A}(t-1) = 1 \text{ for } t=1,2
\]
\item[-] Positivity assumption:
\[
P_0(A(t) = 1, C(t) = 0 | \bar{L}(t), Y(t)=0, \bar{C}(t-1) = 0, \bar{A}(t-1) = 1) > 0 - a.e. \text{ for } t = 1,2
\]
\end{itemize}



\item Under the above assumptions, $\Psi(P_{U,X}) = \Psi(P_0)$. That is:

\begin{align*}
E_{U,X}[Y(3)_{\bar{a}(2)=1, \bar{c}(2)=0}] = & \sum_{\bar{l}(2), y(2)} \Big{[}P_0(Y(3) = 1 | \bar{A}(2) = 1, \bar{C}(2) = 0, \bar{L}(2) = \bar{l}(2), Y(2) = y(2)) \\
& \times P_0(L(2) = l(2) | Y(2) = y(2), A(1) = 1, C(1) = 0, L(1) = l(1)) \\
& \times P_0(Y(2) = y(2) | A(1) = 1, C(1) = 0, L(1) = l(1)) \\
& \times P_0(L(1) = l(1)) \Big{]}\\
= & \sum_{\bar{l}(2)} \Big{[}\big{[}P_0(Y(3) = 1 | \bar{A}(2) = 1, \bar{C}(2) = 0, \bar{L}(2) = \bar{l}(2), Y(2) = 1) \\
& \times P_0(L(2) = l(2) | Y(2) = 1, A(1) = 1, C(1) = 0, L(1) = l(1)) \\
& \times P_0(Y(2) = 1 | A(1) = 1, C(1) = 0, L(1) = l(1)) \\
& \times P_0(L(1) = l(1))\big{]} \\
& + \big{[}P_0(Y(3) = 1 | \bar{A}(2) = 1, \bar{C}(2) = 0, \bar{L}(2) = \bar{l}(2), Y(2) = 0) \\
& \times P_0(L(2) = l(2) | Y(2) = 0, A(1) = 1, C(1) = 0, L(1) = l(1)) \\
& \times P_0(Y(2) = 0 | A(1) = 1, C(1) = 0, L(1) = l(1)) \\
& \times P_0(L(1) = l(1))\big{]}\Big{]} \\
= & \sum_{\bar{l}(1)} P_0(Y(2) = 1 | A(1) = 1, C(1) = 0, L(1) = l(1)) \\
& \times P_0(L(1) = l(1)) ]\\
& + \sum_{\bar{l}(2)} [P_0(Y(3) = 1 | \bar{A}(2) = 1, \bar{C}(2) = 0, \bar{L}(2) = \bar{l}(2), Y(2) = 0) \\
& \times P_0(L(2) = l(2) | Y(2) = 0, A(1) = 1, C(1) = 0, L(1) = l(1)) \\
& \times P_0(Y(2) = 0 | A(1) = 1, C(1) = 0, L(1) = l(1)) \\
& \times P_0(L(1) = l(1))]
\end{align*}

To understand the third equality, note that if $Y(2) = 1$ (e.g., the student becomes ill at time 1), $Y(3)$ and $L(2)$ will deterministically be equal to some value.

For illustration, consider the case where $L(2)$ is defined as deterministically taking value 0 after the event occurs. You can work out for yourself that this equality also holds under different ways of deterministically assigning a value to $L(2)$ after the event, such as equal to its last observed value. 

Thus, $P_0(Y(3) = 1 | \bar{A}(2) = 1, \bar{C}(2) = 0, \bar{L}(2) = \bar{l}(2), Y(2) = 1) = 1$ and for a given value of $l(1)$, $P_0(L(2) = l(2) | Y(2) = 1, A(1) = 1, C(1) = 0, L(1) = l(1))$ will be equal to 1 for $l(2) = 0$ and equal to 0 for all possible values of $l(2)$. Note that it doesn't matter what value we set $L(2)$ equal to deterministically when $Y(2) = 1$; however, when we set it, the g-computation formula reduces to the above formula. 

\end{enumerate}
\end{solution}

\pagebreak
\section{For Your Project: Identification}

Think through the following questions and apply them to the dataset you will use for your final project.

\begin{enumerate}
\item Under what assumptions is the target causal parameter you came up with in the previous lab identified as a function of the observed data distribution?
\item What is your $\Psi(P_0)$, the statistical estimand?
\item Optional: confirm that in your simulation, the value of your estimand equals the value of your target causal parameter.
\end{enumerate}

\pagebreak

\section{Feedback}

Please attach responses to these questions to your lab. Thank you in advance!

\begin{enumerate}
\item Did you catch any errors in this lab? If so, where?
\item What did you learn in this lab?
\item Do you think that this lab met the goals listed at the beginning? 
\item What else would you have liked to review? What would have helped your understanding?
\item Any other feedback?
\end{enumerate}



\end{document}


